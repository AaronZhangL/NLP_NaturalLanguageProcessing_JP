#
#########################################################
# 自然言語処理解析システム
#########################################################
#
#  自然言語処理によるテキスト自動要約 
#                                                       一般社団法人  共同通信社
#                                                       情報技術局将来技術開発室
#                                                                   鈴木  維一郎
#
#  要旨: Webで必要な情報を見つけるという作業は悪夢である。Webで特定の情報を検索
#  していると、膨大な無関係の資料に途方に暮れ、肝心の検索している事項を見落とし
#  てしまう事がよくある。検索は不正確で、多くの場合、何千ものページへのポインタ
#  が返される。さらに求める情報を得るためには、検索された文書全体を読まなければ
#  ならない。本当に関連のあるWebページを見つける事ができたとしても、ページ内の検
#  索が難しかったり、情報が明確でなかったりする場合がある。
#
#   "Author"  suzuki.iichiro@kyodonews.jp  "perlMan" horiuchi.kotaro.ani
#   "pipeMan" kanazawa.hayato  "awkMan"  sugai.takayuki "graphMan" akasaki.haruka
#
#  インターネットに情報が溢れる現在、Webから必要とする情報を効率よく探し出したり、
#  Web上に分散した情報を組み合わせ集約し、自動的に処理したりするための技術が求め
#  られている。こうしたセマンティックWebの世界では、人工知能分野で長く研究されて
#  きたオントロジーの研究を取り入れている。こうした技術により、Webは巨大な知識ベ
#  ースとなり、様々な問題に対して推論を行い、有益な結論を導く事ができるようになる。
#
#  こうした技術により、Webは機械が理解可能な情報となり、標準によってきちんと定義
#  され、人と情報の世界においてある種の均衡状態となる。そのためには情報の把握を
#  手助けしてくれる、情報に関する情報「メタデータ」が必要である。
#
#  文書内から「メタデータ」を効率良く抽出することで、機械が文書の構造および意味
#  的義を理解し、行間の感情表現を特定し、文書の文脈を理解し、要旨／要約を自動生
#  成する事が可能となる。
#
#  テキスト自動要約は以下の自然言語処理技術が必要とされる
#
#    １．形態素解析  文中の単語を分解し品詞を特定する。（mecab)
#    ２．構文解析  文中の単語間の構文的関係を決定し文の構造を決定する。(Cabocha)    
#    ３．意味解析  単一テキスト内の構造（文脈）を決定する
#    ４．照応解析  代名詞、定名詞句、「こそあど」と言われる指示詞などが文脈中の
#                  別の単語と同じ対象を指示する言語対照テキストの構造の決定、
#                  照応の解析、省略の補完。
#
#  我々は、新聞を読むとき、まず記事の見出しを見てあたりをつけ、見出しに添えられた
#  要旨を読み、その記事を読むかどうかを判断していると思われる。これは、見出しが、
#  記事の内容を簡潔に示す「超要約」、要旨が記事本文の「要約」になっているからだと
#  言える。
#  
#
#  要約実現へののステップ
#    １．何らかの情報を元にして、各文の重要度を計算する
#    ２．重要度が上位の文から順に、指定された要約率に達するまで文を選択する。
#
#
#  アプローチ
#      テキスト中の単語/文章の[重要度]を利用する
#      要約対象の文書中に現れる単語の頻度を計算し、その順に重要語を定義する方法。頻
#      度が多い語は、その文章の主題に大きく関係している語であるとする仮定の下に、
#      主に名詞の頻度を計算し、その語が多く含まれる文を重要文として抽出する方法がと
#      られている。重要文抽出法のデメリットとしてテキストが複数の話題を含む場合に問
#      題が生じる事が指摘されている。
#
#
# ##############################################################################   
#  1. 重要語の抽出ロジック 以下のロジックの組み合わせにより要約を実現する
#     a)名詞の頻度                FRQ
#     b)複合名詞の頻度            TF
#     c)名詞の延べ数              LRTOTAL
#     d)名詞の異なり数            LRUNIQUE
#     e)複合語のパープレキシティ  PP
#     f)機械学習                  STAT
#
#
#  a) Frequency(名詞の頻度)による重要度計算                      NOLR FRQ
#     連接情報を使わずに名詞の頻度で重み付け#       calc_imp_by_HASH_Freq 
#   <>LR=0        隣接情報を使わない(NOLR)
#     LR=1    隣接情報の延べ数を使う(LRTOTAL)
#     LR=2  隣接情報の異なり数を使う(LRUNIQUE)
#     LR=3    パープレキシティをみる(LRPP)
#     frq=0 名詞の頻度を使用しない
#   <>frq=1               名詞の頻度(FRQ)
#     frq=2   複合名詞内の名詞の頻度(TF)
#   <>stat_mode=0 機械学習しない
#     stat_mode=1 機械学習する
#
#
#  b)Term Frequency(複合名詞の頻度)による重要度計算             NOLR TF 
#    連接情報を使わずに複合名詞の頻度で重み付け     calc_imp_by_HASH_TF   
#   <>LR=0        隣接情報を使わない(NOLR)
#     LR=1    隣接情報の延べ数を使う(LRTOTAL)
#     LR=2  隣接情報の異なり数を使う(LRUNIQUE)
#     LR=3    パープレキシティをみる(LRPP)
#     frq=0  名詞の頻度を使用しない
#     frq=1               名詞の頻度(FRQ)
#   <>frq=2   複合名詞内の名詞の頻度(TF)
#   <>stat_mode=0 機械学習しない
#     stat_mode=1 機械学習する
#
#    重要度計算において、複合語の重みを使わず、重要度を用語の出現頻度で計算する。
#    ただし、用語が他の用語の一部として現れた場合もカウントする。例えば、「大学 
#    野球」の使用回数が２回だとして、他に「大学 野球 秋季 リーグ」が１回 「大学 
#    野球 リーグ」が２回使われていると、「大学 野球」は「大学 野球 秋季 リーグ」
#   「大学 野球 リーグ」の一部を構成しているので、「大学 野球」の２回の他「大学 
#    野球 秋季 リーグ」１回、 「大学 野球 リーグ」２回を足し頻度５とする  
#
#
#  c)連接情報＋各単名詞の[延べ数]による重要箇所抽出         LRTOTAL FRQ
#                                                      calc_imp_by_HASH 
#    LR=0            隣接情報を使わない(NOLR)
#  <>LR=1              隣接情報の延べ数(LRTOTAL) 
#    LR=2      隣接情報の異なり数を使う(LRUNIQUE)
#    LR=3        パープレキシティをみる(LRPP)
#    frq=0       名詞の頻度を使用しない
#  <>frq=1                   名詞の頻度(FRQ)
#    frq=2       複合名詞内の名詞の頻度(TF)
#  <>stat_mode=0         機械学習しない
#    stat_mode=1               学習する
#
#  重要度計算において、連接語の重みを連接した単語の延べ数で計算する。例えば、統
#  計データで「大学」という語が「野球」の前に２回、「駅伝」の前に３回連接したと
#  すると。連接語の重みは次のとおり計算される。＊「大学」:５回 （「大学野球」２
#  回  ＋  「大学駅伝」３回） 
#
#
#
#
#  e)連接情報＋各単名詞の[異なり数]による重要箇所抽出   LRUNIQUE FREQ
#                                                       calc_imp_by_HASH 
#    LR=0            隣接情報を使わない(NOLR)
#    LR=1              隣接情報の延べ数(LRTOTAL) 
#  <>LR=2      隣接情報の異なり数を使う(LRUNIQUE)
#    LR=3        パープレキシティをみる(LRPP)
#    frq=0       名詞の頻度を使用しない
#  <>frq=1                   名詞の頻度(FREQ)
#    frq=2       複合名詞内の名詞の頻度(TF)
#  <>stat_mode=0         機械学習しない
#    stat_mode=1               学習する
#
#
#
#
#  f)連接情報＋各単名詞のエントロピーのべき乗の合計による重要箇所抽出
#                                                     LRPP TF
#                                                     calc_imp_by_HASH_PP 
#    LR=0            隣接情報を使わない(NOLR)
#    LR=1              隣接情報の延べ数(LRTOTAL) 
#    LR=2      隣接情報の異なり数を使う(LRUNIQUE)
#  <>LR=3        パープレキシティをみる(LRPP)
#    frq=0       名詞の頻度を使用しない
#    frq=1                   名詞の頻度(FREQ)
#  <>frq=2       複合名詞内の名詞の頻度(TF)
#  <>stat_mode=0         機械学習しない
#    stat_mode=1               学習する
#
#  重要度計算において、複合語の重みをパープレキシティで計算する。パープレキシティ
#  は情報理論で使われる指標で、本システムでは各単名詞に「情報理論的に見ていくつの
#  単名詞が連接可能か」を示している。これは、以下のようにして求まる単名詞のエント
#  ロピーを元に、２のべき乗することで求められる。連接する語のそれぞれの出現確率を
#  P1～Pnとおくと、エントロピーの計算は次のように示せる。なお対数の底は２である。
#  ＊(-1 * P1 * log(P1)) + (-1 * P2 * log(P2)) ....... + (-1 * Pn * log(Pn))
#  例えば、統計データで、「大学」という語が「野球」の前に２回、「駅伝」の前に３
#  回連接（あわせると計５回連接）したとすると。単名詞のエントロピーは次のとおりに
#  なる。出現確率は「大学野球」が 2/5, 「大学駅伝」が 3/5である。 
#  ＊(-2/5 * log(2/5)) + (-3/5 * log(3/5))
#   「大学」13回   （「大学野球」2^2回  ＋  「大学駅伝」3^2回）
#
#
#  g)機械学習機能（連接語統計情報の蓄積）による重要箇所抽出   LR F STAT    
#                                                             calc_imp_by_DB 
#    LR=0            隣接情報を使わない(NOLR)
#  <>LR=1              隣接情報の延べ数(LRTOTAL)
#  <>LR=2      隣接情報の異なり数を使う(LRUNIQUE)
#    LR=3        パープレキシティをみる(LRPP)
#  <>frq=0       名詞の頻度を使用しない
#  <>frq=1                   名詞の頻度(FRQ)
#  <>frq=2       複合名詞内の名詞の頻度(TF)
#    stat_mode=0         機械学習しない
#  <>stat_mode=1               学習する(STAT)
#
#
#
# ##############################################################################   
#  2. 要約文抽出のロジック
#      テキスト中の文あるいは単語間のつながりと関係性情報を利用する。
#      テキスト中の単語がノード、その関係をリンク、またはテキスト中の文をノード、
#      文間の関係をリンクで表現したグラフでテキストを表現し、このグラフ中での活性値
#      の伝搬により、高い活性値を得た単語、句、文を重要と見なす重要文抽出手法
#
#     lib/calcImp.sh
#     1．テキスト中の単語の重要度を利用する  
#       a)名詞の頻度                FRQ      calc_imp_by_HASH_Freq ;
#       b)複合名詞の頻度            TF       calc_imp_by_HASH_TF 
#       c)名詞の延べ数              LRTOTAL  calc_imp_by_HASH
#       d)名詞の異なり数            LRUNIQUE calc_imp_by_HASH
#       e)複合語のパープレキシティ  PP       calc_imp_by_PP ;
#       f)機械学習                  STAT     calc_imp_by_DB ;
#
#     summaryExtract.calc_imp_by_HASH_ontology();
#     2．テキスト中の文あるいは単語間のつながりと関係性情報を利用する。
#       <>ノードによる重み付け        
#         (1)単語毎の重要度：上記重要語の抽出アルゴリズムを用いて算出  
#         (2)人名／地名／組織／日付の重要度：単語毎に1を加算する       
#         (3)係り受けの数：オントロジー解析の結果より算出した単語毎に借り
#            受けられた数を算出                                        
#
#     3．テキスト中あるいは段落中での文の位置情報を利用する
#       <>エッジによる重み付け
#         (4)係り受けの重要度：cabochaコマンドで出力されたものを用いる 
#     4．見出し、タイトルなどの情報を利用する
#         (5)タイトル、見出しを元にした重要度：タイトル、見出しより単語毎
#            の重要度を加算する                                        
#     5．テキスト中の文間の関係を解析したテキスト構造を利用する。
#         (6)文の位置情報による重要度：記事の前の方に出ている語の重要度を
#            上げ、文の重要度を計算する
#         (7)意見評価表現を含むエッジのスコアを加算する
#         (8)単語間のつながりと関係性情報の重要度：高い活性値を得た単語、
#            句、文を重要と見なし加算する
#
#     6. オントロジー
#      　システムは内容を理解した上で、重要な部分を要約しているわけではない。内容の理
#      解そのものが、現在のコンピュータで難しい事が原因である。構文解析や意味解析の手
#      法が不足しているだけでなく、人間の持っている常識や当該分野の知識などが不足して
#      いるのである。このような知識の不足に対して、オントロジーの研究が進められている。
#      これは概念辞書とも言われるもので、人間の持っている知識を構造化してコンピュータ
#      で扱えるよう電子化したデータベースである。＜いまここ＞
# 
##########################################
# 実行に必要なライブラリなど
# libc/libglibのインストール
#
# #linux
# yum install libc.so.6 ;
# yum install libglib-2.0.so.0
#
# #mac 
# sudo port install glib2
#
########
# gtk2/pkgconfigのインストール
#
# sudo port install gtk2 ;
# sudo port install pkgconfig ;
#
#
########
# gcc/g++ installのインストール
#
# yum install gcc
# yum install gcc-c++
#
########
# mecab/mecab dic のインストール
#
# mecabのインストール
# wget http://code.google.com/p/mecab/downloads/detail?name=mecab-0.996.tar.gz
# tar -zxvf mecab-0.996.tar.gz
# cd mecab-0.996
# ./configure 
# make
# make check 
# make install 
# ln -s /usr/local/bin/mecab /usr/bin/mecab ;
#
# 形態素解析 mecab dicのインストール
# mecab-ipadicのインストール
# wget https://mecab.googlecode.com/files/mecab-ipadic-2.7.0-20070801.tar.gz
# tar -zxvf mecab-ipadic-2.7.0-20070801.tar.gz
# cd mecab-ipadic-2.7.0-20070801
# ./configure 
# make 
# make check 
# make install 
#
########
# darts のインストール
#
# tar xzvf darts-0.3.tar.gz
# cd darts-0.3
# make clean 
# ./configure
# make clean ;
# make
# make check
# make install
# 
#
########
# TinySVMのインストール
#
# # sudo port install tinysvm
# #でもよい。
# tar xzvf TinySVM-0.09.tar.gz
# cd TinySVM-0.09
# make clean 
# ./configure
# make clean ;
# make
# make check
# make install
# 
# # mac
# # sudo port install tinysvm
#
########
# YumChaのインストール
#
# tar xzvf yamcha-0.33.tar.gz
# cd yamcha-0.33
# make clean 
# ./configure
# make
# make check
# make install
# cd ../ ;
# ln -s /usr/local/bin/yamcha /usr/bin/yamcha ;
#
# コンパイルエラー発生
# error: strlen was not declared in this scope
#
# 対応は以下の通り。
# $ vim src/common.h
# #include < string.h >  (追加)
#
# コンパイルエラー発生
# error: atoi is not a member of ‘std’
#
# 対応は以下の通り。
# $ vim libexec/mkdarts.cpp
# #include < cstdlib >
#
# つまるところ、gcc 4.2 以前と gcc 4.3 の違いに起因するようです。
# ggcc 4.2 以前では、string.h や stdlib.h のようなヘッダファイルは
# 明示的にインクルードしていなくても、そのヘッダファイルでプロトタイプ宣言されている関数を呼び出すことができました。
# gcc 4.3 では明示的なインクルードが必要です。
# とりあえず、 src/common.h に #include <string.h> を追加して make してみます
#
########
# CRF++のインストール
#
# tar xvf CRF++-0.58.tar.gz
# cd CRF++-0.58
# make clean 
# ./configure
# make
# make install
# ln -s /usr/local/bin/crf_learn /usr/bin/crf_learn ;
# ln -s /usr/local/bin/crf_test /usr/bin/crf_test ;
# ldconfig
#
########
# cabochaのインストール
#
# tar zxvf  cabocha-0.60.tar.gz
# cd cabocha-0.60
# 
# # デフォルトはEUC です。
# make clean 
# LIBS=-liconv ./configure --with-posset=IPA
# make
# make install
# ln -s /usr/local/bin/cabocha /usr/bin/cabocha
# ldconfig
#
# コンパイルエラー発生
# error: strlen was not declared in this scope
#
# 対応は以下の通り。
# $ vim src/common.h
# #include <string.h>  (追加)
#
# コンパイルエラー発生
# error: ‘::unlink’ has not been declared
#
# 対応は以下の通り。
# $ vim src/utils.cpp
# #include <unistd.h>  (追加)
#
# コンパイルエラー発生
# chunk_learner.cpp:6:10: fatal error: 'crfpp.h' file not found
# コマンドライン上で、xcode-select --installを実行
# xcodeのコマンドラインツールをインストールされます。
#
# 対応は以下の通り。
# /usr/local/include/crfpp.h  があることを確認する
# .bash_profileでC_INCLUDE_PATHを設定する
# vim /Users/root/.bash_profile
# export C_INCLUDE_PATH=/usr/local/include:$C_INCLUDE_PATH
# 下記で確認する
# echo $C_INCLUDE_PATH
#
########
# jumanのインストール
#
# tar zxvf juman-6.01.tar.gz ;
# cd juman-6.01 ;
# ./configure ;
# make clean ;
# make ;
# make install ;
# ln -s /usr/local/bin/juman /usr/bin/juman ;
# ldconfig  ;
# 
########
# tinycdbのインストール
#
# tar -zxvf tinycdb-0.78.tar.gz ;
# cd tinycdb-0.78
# make clean ;
# make
# make install ;
# ln -s /usr/local/bin/cdb /usr/bin/cdb ;
# ldconfig  ;
#
########
# KNPのインストール
#
#tar -zxvf knp-3.01.tar.gz ;
#cd knp-3.01
#./configure ;
#make clean ;
#make ;
#make install ;
#ln -s /usr/local/bin/knp /usr/bin/knp ;
#ldconfig  ;
#
#make 実行時  下記エラーの場合
#  /bin/sh: line 0: ulimit: stack size: cannot modify limit: Invalid argument
# vim dict/ebcf/Makefile
# SsをSへ書き換える
#
########
# 意見（評価表現）抽出ツールのインストール
#
# #tar -zxvf extractopinion-1.2.tar.gz ;
# # ra カレントディレクトリで
# mv extractopinion-1.2 ../ ;
# cd ../extractopinion-1.2 ;
# cd svmtools
# make clean ;
# make ;
# cd ../pol ;
# make clean ;
# make ;
# ldconfig  ;
#
# #Apacheで動作させるために必要(whichにcrf_testが通っていなければ)
# ln -s /usr/local/bin/crf_test /usr/bin/crf_test ;
# ./extract.sh in8 ;
#
#
########
# macab-jumandicのインストール
#
# $ cd gzArchive 
# $ cd mecab-jumandic-5.1-20070304
# $ ./configure 
# $ make 
# $ sudo make install 
#
########
# JdepP
#
# $ cd gzArchive 
# $ tar -zxvf jdepp-latest.tar.gz
# $ cd jdepp-2015-02-08 
# $ ./configure --with-corpus=kyoto-partial --disable-autopos-train
# $ make 
# $ sudo make install 
# $ ln -s /usr/local/bin/jdepp /usr/bin/jdepp
# $ make model ;
# $ make install ;
#
######################################################


